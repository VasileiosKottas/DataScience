{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather articles from wikipedia to compare the similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the libraries\n",
    "import concurrent.futures\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_article(wiki_article, timeout=10): # This function gathers the websites from wiki\n",
    "    '''\n",
    "    Method that gets a Wikipedia page. \n",
    "    '''\n",
    "    wiki_page_url = 'https://en.wikipedia.org/wiki/'+wiki_article\n",
    "    response = requests.get(url=wiki_page_url, timeout=timeout)\n",
    "    article = response.text\n",
    "    return article\n",
    "\n",
    "\n",
    "def get_wikipedia_articles(wiki_articles):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = []\n",
    "        for article in wiki_articles:\n",
    "            futures.append(executor.submit(get_wikipedia_article,\n",
    "                                           wiki_article=article)\n",
    "                          )\n",
    "        articles = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            articles.append(future.result())\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Let's define some functions to clean HTML code, punctuation marks and symbols from the original articles\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def clean_punctuation_marks(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ', text)\n",
    "\n",
    "def clean_return_and_tabs(text):\n",
    "    return re.sub(r'[\\n, \\t]',' ', text)\n",
    "\n",
    "def clean_article(data):\n",
    "    cdata = clean_html(data)\n",
    "    cdata = clean_punctuation_marks(cdata)\n",
    "    cdata = clean_return_and_tabs(cdata)\n",
    "    return cdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = [\n",
    "    'lion',\n",
    "    'beetle',\n",
    "    'tiger',\n",
    "]\n",
    "articles = get_wikipedia_articles(wiki_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the articles, let's do some basic cleaning and NLP on them:\n",
    "\n",
    "Basic cleaning -> tokenise -> remove stopwords -> stem -> remove numbers -> remove specific stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = tokenize.NLTKWordTokenizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make it all lowercase and do a basic cleaning\n",
    "articles = map(lambda a: a.lower(), articles)\n",
    "articles = map(clean_article, articles)\n",
    "# Then we tokenise the articles, extracing the words from them\n",
    "articles = map(tokeniser.tokenize, articles)\n",
    "\n",
    "# We will also remove the English stopwords \n",
    "\n",
    "stopwords = set(['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', \n",
    "                'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', \n",
    "                'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', \n",
    "                'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', \n",
    "                'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "                'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', \n",
    "                'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', \n",
    "                'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', \n",
    "                'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', \n",
    "                'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', \n",
    "                'doing', 'it', 'how', 'further', 'was', 'here', 'than'])\n",
    "\n",
    "def remove_stopwords(document, stopwords):\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if word not in stopwords:\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(lambda a: remove_stopwords(a, stopwords), articles)\n",
    "\n",
    "# And then we stem those words to remove plurals and other variations\n",
    "articles = [map(stemmer.stem, article) for article in articles]\n",
    "\n",
    "\n",
    "def remove_numbers(document):\n",
    "    numbers = set(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if not any(num in word for num in numbers):\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(remove_numbers, articles)\n",
    "\n",
    "def remove_wiki_and_wg_words(document):\n",
    "    output_doc = []\n",
    "    for word in document:\n",
    "        if not word.startswith('wg') and not word.startswith('wiki'):\n",
    "            output_doc.append(word)\n",
    "    return output_doc\n",
    "\n",
    "articles = map(remove_wiki_and_wg_words, articles)\n",
    "articles = list(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this algorithm we can see the following:\n",
    "* TF: How relevant a term is inside the article. TF = num. times term appears in article/num. terms in document\n",
    "* IDF: Find the relevance of a term across all the documents. IDF = Log(num. documents in corpus/ num. documents where the term is in)\n",
    "\n",
    "Term relevance score = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(articles, word):\n",
    "    N = len(articles)\n",
    "    occurance = len([token for token in articles if token == word])\n",
    "    \n",
    "    return occurance / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word, corpus):\n",
    "    count_of_documents = len(corpus) + 1\n",
    "    count_of_documents_with_word = sum([1 for doc in corpus if word in doc]) + 1\n",
    "    idf = np.log10(count_of_documents/count_of_documents_with_word) + 1\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Find the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiger', 'mw', 'parser', 'output', 'n', 'doi', 'j', 'p', 'vector', 'toc', 'tigri', 'm', 'panthera', 'popul', 'conserv', 'b', 'rang', 'cat', 'speci', 'hlist']\n",
      "['lion', 'mw', 'parser', 'output', 'toc', 'vector', 'p', 'doi', 'j', 'panthera', 'nation', 'africa', 'male', 'leo', 'list', 'm', 'african', 'hlist', 'cat', 'c']\n",
      "['beetl', 'mw', 'parser', 'output', 'toc', 'vector', 'speci', 'insect', 'doi', 'coleoptera', 'list', 'level', 'class', 'edit', 'item', 'use', 'larva', 'li', 'hlist', 'id']\n"
     ]
    }
   ],
   "source": [
    "def calculate_tf_idf(articles):\n",
    "    sorted_lists = []\n",
    "    \n",
    "    for article in articles:\n",
    "        tf_idf_dict = {}\n",
    "        for word in article:\n",
    "            tf = term_frequency(article, word)\n",
    "            idf = inverse_document_frequency(word, article)\n",
    "            tf_idf_dict[word] = tf * idf\n",
    "        \n",
    "        sorted_words = sorted(tf_idf_dict, key=tf_idf_dict.get, reverse=True)[:20]\n",
    "        sorted_lists.append(sorted_words)\n",
    "    \n",
    "    return sorted_lists\n",
    "\n",
    "# Example usage:\n",
    "articles = [articles[0], articles[1], articles[2]]  # Assuming articles is a list containing 3 lists\n",
    "shorted = calculate_tf_idf(articles)\n",
    "for sorted_words in shorted:\n",
    "    print(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between tiger and lion : 0.4286\n",
      "Similarity between tiger and beetl : 0.25\n",
      "Similarity between lion and beetl : 0.25\n"
     ]
    }
   ],
   "source": [
    "def jaccard(a1, a2):\n",
    "    set1 = set(a1)\n",
    "    set2 = set(a2)\n",
    "    union_size = len(set1.union(set2))\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    return round(intersection_size/union_size, 4)\n",
    "\n",
    "for i in range(len(shorted)-1):\n",
    "    for j in range(i+1, len(shorted)):\n",
    "        print('Similarity between', shorted[i][0], 'and', shorted[j][0], ':',jaccard(shorted[i], shorted[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii  # To transform (hash) each shingle to a 32-bit integer \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle_article(shorted, shingle_size=3):\n",
    "    hashed_shingles = [] # Shingles are those 3 words transformed in an integer using CRC32 algorithm\n",
    "    for i in range(len(shorted)-(shingle_size-1)):\n",
    "        # We put groups (shingles) of 3 words together, separated by a space\n",
    "        # We could change the number of words per shingle, changing shingle_size\n",
    "        shingle = ' '.join([shorted[i+j] for j in range(shingle_size)])\n",
    "        crc_hash = binascii.crc32(bytes(shingle.encode('UTF-8')))\n",
    "        hashed_shingles.append(crc_hash)\n",
    "    return hashed_shingles\n",
    "\n",
    "SHINGLE_SIZE = 1\n",
    "\n",
    "# We \"save\" the first word of the article in order to be able to idenfity it later when we hash it all \n",
    "shingled_articles = map(lambda a: (a[0], shingle_article(a, shingle_size=SHINGLE_SIZE)), shorted)\n",
    "shingled_articles = list(shingled_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCoefficient = 2**32-1  # Because we are transforming shingles into 32-bit integers, we know their maximum\n",
    "big_prime = 4294967311  # And we found their next primer in internet\n",
    "\n",
    "N = 20 # Using 20 hashing functions\n",
    "\n",
    "# We will use N hashing functions, so we need to randomly take N As and N Bs for our N hashing functions\n",
    "# Our hashing functions will be of the form: hashed_value = A * value + B % big_prime\n",
    "a_coeffs = [random.randint(0, maxCoefficient) for i in range(N)]\n",
    "b_coeffs = [random.randint(0, maxCoefficient) for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature for tiger :  [38153704, 120609307, 83707228, 106787783, 275718947, 602122745, 152872557, 34995198, 584691928, 135974804, 62540410, 383057044, 608270832, 178204528, 461075123, 343208120, 197815915, 122461160, 435865575, 286825418]\n",
      "Signature for lion :  [750223927, 46802055, 508356304, 40027168, 55406979, 602122745, 160166190, 34995198, 178019522, 135974804, 62540410, 383057044, 313620813, 338554930, 241323024, 56378505, 197815915, 83689526, 77145839, 535766859]\n",
      "Signature for beetl :  [438825287, 763506739, 508356304, 118347302, 487100142, 83334534, 63481854, 361993534, 99872824, 135974804, 62540410, 441775863, 99724279, 19631363, 145730619, 268221364, 141678703, 122461160, 187706360, 60918832]\n"
     ]
    }
   ],
   "source": [
    "def get_hashed_value(shingle, a_coeff, b_coeff, big_prime):\n",
    "    return ((shingle * a_coeff) + b_coeff) % big_prime\n",
    "\n",
    "# For each article, and each of the N hash functions, we calculate the hashes of all the shingles\n",
    "# and get the minimum of those hashes. This will create a signature of size N for each document. \n",
    "# Finally, to estimate their Jaccard similarity, we just only compare the signatures\n",
    "article_signatures = {}\n",
    "article_names = []\n",
    "for article_name, shingled_article in shingled_articles:\n",
    "    article_signature = []\n",
    "    for hashing_function_index in range(N):\n",
    "        a_coeff = a_coeffs[hashing_function_index]\n",
    "        b_coeff = b_coeffs[hashing_function_index]\n",
    "        hashes = map(lambda shingles: get_hashed_value(shingles, a_coeff, b_coeff, big_prime), shingled_article)\n",
    "        article_signature.append(min(list(hashes)))\n",
    "    article_signatures[article_name] = article_signature\n",
    "    article_names.append(article_name)\n",
    "    \n",
    "for s in article_signatures:\n",
    "    print('Signature for', s,': ',article_signatures[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 hashing functions, and shingles of size 1 , MinHas produces the following results:\n",
      "Similarity between tiger and lion : 0.1765\n",
      "Similarity between tiger and beetl : 0.0811\n",
      "Similarity between lion and beetl : 0.0811\n"
     ]
    }
   ],
   "source": [
    "print('Using', N, 'hashing functions, and shingles of size',SHINGLE_SIZE, ', MinHas produces the following results:')\n",
    "for i in range(len(article_names)-1):\n",
    "    for j in range(i+1, len(article_names)):\n",
    "        print('Similarity between', article_names[i], 'and', article_names[j], ':',\n",
    "              jaccard(article_signatures[article_names[i]], article_signatures[article_names[j]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
